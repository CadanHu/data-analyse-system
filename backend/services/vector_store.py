import os
from typing import List, Dict, Any, Optional
from pathlib import Path
from langchain_community.vectorstores import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document

class VectorStore:
    """å‘é‡å­˜å‚¨æœåŠ¡ï¼šè´Ÿè´£æ–‡æ¡£ç´¢å¼•ä¸æ£€ç´¢"""

    def __init__(self, persist_dir: str = "backend/data/vector_db"):
        self.persist_dir = persist_dir
        # é…ç½® HuggingFace é•œåƒï¼Œé˜²æ­¢è¿æ¥è¶…æ—¶å¯¼è‡´å¯åŠ¨é˜»å¡
        os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
        
        try:
            print("â³ [VectorStore] æ­£åœ¨åŠ è½½åµŒå…¥æ¨¡å‹...")
            self.embeddings = HuggingFaceEmbeddings(
                model_name="sentence-transformers/all-MiniLM-L6-v2",
                model_kwargs={'device': 'cpu'}
            )
            print("âœ… [VectorStore] åµŒå…¥æ¨¡å‹åŠ è½½æˆåŠŸ")
        except Exception as e:
            print(f"âš ï¸ [VectorStore] åµŒå…¥æ¨¡å‹åŠ è½½å¤±è´¥ (å°†å½±å“ RAG åŠŸèƒ½): {str(e)}")
            self.embeddings = None

        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=100
        )
        self.vector_db: Optional[Chroma] = None

    def initialize(self):
        """åˆå§‹åŒ–æˆ–åŠ è½½ç°æœ‰çš„å‘é‡åº“"""
        if not self.embeddings:
            print("âŒ [VectorStore] æ— æ³•åˆå§‹åŒ–å‘é‡åº“ï¼šåµŒå…¥æ¨¡å‹æœªå°±ç»ª")
            return

        if not os.path.exists(self.persist_dir):
            os.makedirs(self.persist_dir)
        
        self.vector_db = Chroma(
            persist_directory=self.persist_dir,
            embedding_function=self.embeddings
        )
        print(f"âœ… [VectorStore] å‘é‡åº“å·²å°±ç»ª: {self.persist_dir}")

    async def add_text(self, text: str, metadata: Dict[str, Any]):
        """å°†è§£æå‡ºçš„æ–‡æœ¬åˆ‡ç‰‡å¹¶å­˜å…¥å‘é‡åº“"""
        if not self.vector_db:
            self.initialize()
        
        docs = [Document(page_content=text, metadata=metadata)]
        split_docs = self.text_splitter.split_documents(docs)
        
        self.vector_db.add_documents(split_docs)
        print(f"ğŸ“¥ [VectorStore] å·²ç´¢å¼• {len(split_docs)} ä¸ªç‰‡æ®µï¼Œæ¥è‡ªæ–‡ä»¶: {metadata.get('filename')}")
        return True

    async def search(self, query: str, top_k: int = 4) -> List[Dict[str, Any]]:
        """æ‰§è¡Œç›¸ä¼¼åº¦æ£€ç´¢"""
        if not self.vector_db:
            self.initialize()
        
        results = self.vector_db.similarity_search(query, k=top_k)
        return [
            {
                "content": doc.page_content,
                "metadata": doc.metadata
            } for doc in results
        ]

    def delete_all(self):
        """æ¸…ç©ºå‘é‡åº“ï¼ˆç”¨äºæµ‹è¯•æˆ–é‡ç½®ï¼‰"""
        if self.vector_db:
            self.vector_db.delete_collection()
            self.initialize()
